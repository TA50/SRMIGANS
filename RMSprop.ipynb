{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","celltoolbar":"Raw Cell Format","colab":{"name":"RMSprop.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"oN-D2oEig4zH","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":342},"executionInfo":{"status":"ok","timestamp":1599404809411,"user_tz":-120,"elapsed":4898,"user":{"displayName":"Awad Osman","photoUrl":"","userId":"10154532610080377025"}},"outputId":"1cc5bc08-0a39-42a4-8cec-016470e2c098"},"source":["! nvidia-smi"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Sun Sep  6 15:06:45 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   48C    P0    29W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zn5XJ2Xb0YKf","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","% cd 'drive/My Drive'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WIZQuG9i74Sa","colab":{}},"source":["import tensorflow as tf \n","import numpy as np\n","from tensorflow.keras.layers import *\n","from matplotlib import pyplot as plt\n","import os\n","import shutil\n","import pickle \n","from tqdm.notebook import tqdm\n","from zipfile import ZipFile\n","from glob import glob\n","import time\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"t8822L5-uUMF"},"source":["# Configs"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gRoVbScMt6Qg","cellView":"form","colab":{}},"source":["#@title Config\n","test_name = \"RMSprop\" #@param [\"Adam\", \"RMSprop\", \"Adagrad\"]\n","batch_size =   32#@param {type:\"number\"}\n","hr_height =  256 #@param {type:\"number\"}\n","hr_width =  256 #@param {type:\"number\"}\n","scale = 4 #@param {type:\"number\"}\n","RESET_CHECKPOINTS = False #@param {type:\"boolean\"}\n","training_path= '/content/Data/' #@param {type:\"string\"}\n","\n","#@title training parameters\n","iterations = 151 #@param {type:\"number\"}\n","evaluation_interval =  5 #@param {type:\"number\"}\n","interval= 10       #@param {type:\"number\"}\n","gen_lr = 1e-4 #@param {type:\"number\"}\n","disc_lr = 1e-4 #@param {type:\"number\"}\n","K = 2 #@param {type:\"number\"}\n","\n","ad_loss_weight = 1e-3 #@param {type:\"number\"}\n","gf = 32 #@param {type:\"number\"}\n","gk =  4#@param {type:\"number\"}\n","\n","#@title image parameters\n","channels  = 1 #@param {type:\"number\"}\n","scale = 4 #@param {type:\"number\"}\n","total_images = 100000 #@param {type:\"number\"}\n","\n","\n","train_len = total_images*3//4 \n","val_len = total_images//4\n","\n","hr_shape = (hr_height , hr_width , channels)\n","lr_height = hr_height // scale\n","lr_width  = hr_width // scale\n","lr_shape  = (lr_height , lr_width , channels )\n","total = train_len//batch_size +1\n","\n"," "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FS37jIC-48uI","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yuZK7YwV2Lh9","colab_type":"code","colab":{}},"source":["\n","#########Create directory for samples and checkpoints and models ######### \n","dir_path= './samples/%s'% test_name\n","os.makedirs(dir_path, exist_ok=True)\n","dir_path= './checkpoints/%s'% test_name\n","os.makedirs(dir_path, exist_ok=True)\n","dir_path= './models/%s'% test_name\n","os.makedirs(dir_path, exist_ok=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"LqEtP89aE1uH"},"source":["# Data preprocessing"]},{"cell_type":"code","metadata":{"id":"4nvLqtb7WBPm","colab_type":"code","colab":{}},"source":["ex_discriptoin = { \n","    'hr': tf.io.FixedLenFeature((), tf.string),\n","    'lr': tf.io.FixedLenFeature((), tf.string)\n","}\n","\n","def parse_example(ex): \n","  example = tf.io.parse_single_example(ex, ex_discriptoin)\n","  hr = example['hr']\n","  lr = example['lr']\n","  hr = tf.io.decode_jpeg(hr, channels=1)\n","  lr = tf.io.decode_jpeg(lr, channels=1)\n","  return hr, lr\n","\n","def normalize(hr, lr): \n","  lr = lr/255\n","  hr = hr/255 \n","  hr = 2*hr-1\n","  return hr, lr\n","\n","record_files = glob('/content/drive/My Drive/tfrecords_data/*')\n","record_ds = tf.data.TFRecordDataset(record_files, num_parallel_reads=tf.data.experimental.AUTOTUNE)\n","val_ds = record_ds.take(val_len)\n","val_ds = val_ds.map(parse_example, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","train_ds = record_ds.skip(val_len).take(train_len)\n","train_ds = train_ds.map(parse_example, num_parallel_calls=tf.data.experimental.AUTOTUNE).map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbS3GHFJ2RfB","colab_type":"code","colab":{}},"source":["def take_elements(n): \n","    hr= []\n","    lr = []\n","    for i, j in train_ds.shuffle(1000).take(n):\n","        hr.append(i)\n","        lr.append(j)\n","    return tf.convert_to_tensor(hr), tf.convert_to_tensor(lr)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rj6-y84jKtc-"},"source":["# Sample Images\n"]},{"cell_type":"code","metadata":{"id":"kplRL-Lt2o72","colab_type":"code","colab":{}},"source":["def compute_metrics(hr, fake):\n","   \n","    # hr and fake must be in range [0,1]\n","    \n","    psnr = tf.image.psnr(hr, fake, 1)\n","    ssim = tf.image.ssim(hr, fake, 1) \n","    \n","    return psnr, ssim\n","\n","def sample_image(epoch, test_name=test_name, save_sample=True): \n","        r, c = 2, 2\n","        imgs_hr, imgs_lr = take_elements(2)\n","\n","        fake_hr = G(imgs_lr)  # output in range [-1, 1]\n","        \n","        ######################calculate metrics###########\n","        fake_hr = 0.5*fake_hr +0.5  # range 0, 1\n","        imgs_hr = 0.5*imgs_hr +0.5  # range 0, 1\n","        \n","        psnr, ssim =compute_metrics(fake_hr, imgs_hr)\n","        \n","        print('PSNR= ', np.mean(psnr))\n","        print('SSIM= ', np.mean(ssim))\n","        \n","        # Save generated images and the high resolution originals\n","        titles = ['Generated', 'Original']\n","        fig, axs = plt.subplots(r, c)\n","        figSize = fig.get_size_inches()*4\n","        fig.set_size_inches(figSize)\n","        cnt = 0\n","        for row in range(r):\n","            for col, image in enumerate([fake_hr, imgs_hr]):\n","                axs[row, col].imshow(image[row,:,:,0] , cmap='gray')\n","                axs[row, col].set_title(titles[col])\n","                axs[row, col].axis('off')\n","            cnt += 1\n","\n","        if save_sample:     \n","          fig.savefig(\"samples/%s/%d.png\" % (test_name, epoch))\n","          # Save low resolution images for comparison\n","          plt.imsave(arr= fake_hr[0,:,:,0],fname=('samples/%s/%d_generated.jpg' % (test_name, epoch)), cmap='gray' )\n","          plt.imsave(arr= imgs_hr[0,:,:,0],fname=('samples/%s/%d_real.jpg' % (test_name, epoch) ), cmap='gray' )\n","        else: \n","          plt.show()\n","        plt.close()\n","        \n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"UX3FAvgNcQtz"},"source":["# **Models**"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EP-3pF6M8fXg"},"source":["## Build Generator"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vOfNEc_E8aSK","colab":{}},"source":["def pixel_shuffle(scale): \n","  return lambda x: tf.nn.depth_to_space(x, scale)\n","\n","\n","def upsample(x_in, num_filters): \n","  lyr = Conv2D(num_filters, 4, padding='same')(x_in)\n","  lyr = Lambda(pixel_shuffle(scale=2))(lyr)\n","  return LeakyReLU()(lyr)\n","\n","def RRDB(lyr, name): \n","  layer1 = Conv2D(gf*2, 3, strides=1, padding='same', name=f'a_{name}')(lyr)\n","  layer1 = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(layer1)\n","  layer1 = LeakyReLU()(layer1)\n","  tmp = Add()([lyr, layer1])\n","  layer2 =  Conv2D(gf*2, 3, strides=1, padding='same', name=f'b_{name}')(tmp)\n","  layer2 = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(layer2)\n","  layer2 = LeakyReLU()(layer2)\n","  tmp = Add()([lyr, layer1, layer2])\n","  layer3 =  Conv2D(gf*2, 3, strides=1, padding='same', name=f'c_{name}')(tmp)\n","  layer3 = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(layer3)\n","  layer3 = LeakyReLU()(layer3)\n","  tmp = Add()([lyr, layer1, layer2, layer3])\n","  layer4 =  Conv2D(gf*2, 3, strides=1, padding='same', name=f'd_{name}')(tmp)\n","  layer4 = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(layer4)\n","  layer4 = LeakyReLU()(layer4)\n","  return Add()([lyr, layer1, layer2, layer3, layer4])\n","def res_block(pre_layer):\n","    lyr = Conv2D(gf, (3,3) , padding='same', strides=1)(pre_layer)\n","    lyr = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(lyr)\n","    lyr = LeakyReLU()(lyr)\n","\n","    lyr = Conv2D(gf, (3,3) , padding='same', strides=1)(lyr)\n","    lyr = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(lyr)\n","    lyr = LeakyReLU()(lyr)\n","\n","\n","    lyr = Conv2D(gf, (3,3) , padding='same', strides=1)(lyr)\n","    lyr = BatchNormalization(epsilon=1e-5, beta_initializer='glorot_normal', gamma_initializer='glorot_normal')(lyr)\n","    lyr = LeakyReLU()(lyr)\n","    return Add()([lyr, pre_layer])\n","\n","  \n","\n","def build_G():  \n","\n","    input_layer = Input(shape= (None, None, channels)) \n","    \n","    tmp = Conv2D(gf*2, (3,3) , padding='same', strides=1)(input_layer)\n","    # extracting basic details:\n","      ####  Branch A \n","    branch_A =  Conv2D(gf, (3,3) , padding='same', strides=1)(input_layer)\n","    branch_A = res_block(branch_A) \n","    branch_A = res_block(branch_A)\n","    branch_A = res_block(branch_A)\n","      #### Branch B\n","    branch_B =  Conv2D(gf, (3,3) , padding='same', strides=1)(input_layer)\n","    branch_B = res_block(branch_B) \n","    branch_B = res_block(branch_B)\n","    branch_B = res_block(branch_B)\n","\n","    layer = Concatenate()([branch_A, branch_B])\n","    layer = RRDB(layer, '1')\n","    layer = RRDB(layer, '2')\n","\n","    layer = Add()([tmp, layer])\n","    layer = upsample(layer, gf*4)\n","    layer = upsample(layer, gf*4)\n","    layer = Conv2D(1, 9, padding='same', activation='tanh')(layer)\n","    return tf.keras.models.Model(input_layer, layer)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"FfyyL2sM8i7H"},"source":["## Build Discriminator"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AAiHfJpi_MS-","colab":{}},"source":["def build_D():\n","    # seems to have vanishing gradients , try to replace relu with LeakyRelu\n","    input_layer= Input(shape=hr_shape)\n","    \n","    layer = Conv2D(64, 3, strides=1)(input_layer)\n","    layer = LeakyReLU(0.2)(layer)\n","    \n","    layer = Conv2D(64, 3, strides=2)(layer)\n","    layer = BatchNormalization()(layer)\n","    layer = LeakyReLU(0.2)(layer)\n","    \n","    layer = Conv2D(64*2, 3, strides=1)(layer)\n","    layer = BatchNormalization()(layer)\n","    layer = LeakyReLU(0.2)(layer)\n","    \n","    layer = Conv2D(64*2, 3, strides=2)(layer)\n","    layer = BatchNormalization()(layer)\n","    layer = LeakyReLU(0.2)(layer)\n","    \n","    layer = Conv2D(64*4, 3, strides=1)(layer)\n","    layer = BatchNormalization()(layer)\n","    layer = LeakyReLU(0.2)(layer)\n","    \n","    layer = Conv2D(64*4, 3, strides=2)(layer)\n","    layer = BatchNormalization()(layer)\n","    layer = LeakyReLU(0.2)(layer)\n","    \n","    layer = Conv2D(64*8, 3, strides=1)(layer)\n","    layer = BatchNormalization()(layer)\n","    layer = LeakyReLU(0.2)(layer)\n","    \n","    layer = Conv2D(64*8, 3, strides=2)(layer)\n","    layer = BatchNormalization()(layer)\n","    layer = LeakyReLU(0.2)(layer)\n","    \n","    layer= Flatten()(layer)\n","    layer = Dense(1024)(layer)\n","    layer = LeakyReLU(0.2)(layer)\n","    output_layer = Dense(1)(layer)\n","    return tf.keras.models.Model(input_layer, output_layer) \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EuvfjRQC2yBq","colab_type":"text"},"source":["# Checkpoints config\n"]},{"cell_type":"code","metadata":{"id":"7XCdxRaP239X","colab_type":"code","colab":{}},"source":["def save_checkpoint(step_num, psnr_list, ssim_list, test_name=test_name):\n","    G.save('./checkpoints/{}/generator.h5'.format(test_name))\n","    D.save('./checkpoints/{}/discriminator.h5'.format(test_name))\n","    saving_file= open('./checkpoints/{}/steps_psnr_ssim.pickle'.format(test_name) , 'wb')\n","    pickle.dump((step_num, psnr_list, ssim_list) ,saving_file)\n","    saving_file.close()\n","\n","    G.save(f'./checkpoints/{test_name}/generator_old.h5')\n","    D.save(f'./checkpoints/{test_name}/discriminator_old.h5')\n","    print('#####################checkpoint saved#########################')\n","\n","def load_checkpoint(test_name=test_name):\n","    try:\n","      g=tf.keras.models.load_model(f'./checkpoints/{test_name}/generator.h5')\n","    except OSError:\n","      g=tf.keras.models.load_model(f'./checkpoints/{test_name}/generator_old.h5')\n","    try:\n","      d=tf.keras.models.load_model('./checkpoints/{}/discriminator.h5'.format(test_name))\n","    except OSError:\n","      d=tf.keras.models.load_model('./checkpoints/{}/discriminator_old.h5'.format(test_name))\n","      \n","    saving_file= open('./checkpoints/{}/steps_psnr_ssim.pickle'.format(test_name) , 'rb')\n","    step, psnr_list, ssim_list=pickle.load(saving_file)\n","    saving_file.close()\n","    return  step, psnr_list, ssim_list , g, d\n","   \n","\n","def reset_checkpoint(test_name=test_name): \n","  \n","  dir_path= './samples/%s'% test_name\n","  if os.path.exists(dir_path): \n","    shutil.rmtree(dir_path)\n","    os.mkdir(dir_path)\n","  dir_path= './checkpoints/%s'% test_name\n","  if os.path.exists(dir_path): \n","    shutil.rmtree(dir_path)\n","    os.mkdir(dir_path)\n","  dir_path= './models/%s'% test_name\n","  if os.path.exists(dir_path): \n","    shutil.rmtree(dir_path)\n","    os.mkdir(dir_path)\n","\n","  \n","  save_checkpoint(0, [], [], test_name)\n","  print('reset checkpoint')\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4LE4fSvX3DI4","colab_type":"text"},"source":["### Load checkpoints\n"]},{"cell_type":"code","metadata":{"id":"TM4aSvH-3CQT","colab_type":"code","colab":{}},"source":["if RESET_CHECKPOINTS: \n","  G = build_G()\n","  D = build_D()\n","  reset_checkpoint()\n","\n","last_epoch, psnr_list, ssim_list, G, D= load_checkpoint(test_name)\n","print(last_epoch, psnr_list, ssim_list)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZARNpWJ-E7aG"},"source":["# Losses and optimizers"]},{"cell_type":"code","metadata":{"id":"sStyJLgiAeTN","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"gENS4WqWAPSN","colab":{}},"source":["cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n","mse_loss_fn = tf.keras.losses.MeanSquaredError()\n","def disc_loss(real_logits, fake_logits): \n","    real_labels = tf.zeros_like(real_logits) + 0.05*tf.random.normal(real_logits.shape,mean=0)\n","    fake_labels = tf.ones_like(fake_logits) + 0.05*tf.random.normal(fake_logits.shape,mean=0)\n","    real_loss = cross_entropy(real_labels, real_logits)\n","    fake_loss = cross_entropy(fake_labels, fake_logits)\n","    return 0.5*(real_loss+fake_loss)\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ERc81GeoAhag","colab":{}},"source":["def gen_loss(fake_logits): \n","    # fake logits is the discriminator's decision  about the images came from the generator\n","    return cross_entropy(tf.zeros_like(fake_logits), fake_logits)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rmbdcKIu51pN","colab_type":"text"},"source":["## Optimizer\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kI9WJDz_FCYZ","colab":{}},"source":["if test_name == 'Adam': \n","  disc_opt = tf.keras.optimizers.Adam(disc_lr)\n","  gen_opt = tf.keras.optimizers.Adam(gen_lr)\n","\n","elif test_name == 'Adagrad': \n","  disc_opt = tf.keras.optimizers.Adagrad(disc_lr)\n","  gen_opt = tf.keras.optimizers.Adagrad(gen_lr)\n","\n","elif test_name=='RMSprop':\n","  disc_opt = tf.keras.optimizers.RMSprop(disc_lr)\n","  gen_opt = tf.keras.optimizers.RMSprop(gen_lr)\n","\n","else: \n","  print(\"Error optimizer is not set\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MMqD3eDQFDVA"},"source":["# Training Function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VIp9_3ccAfGf","colab":{}},"source":["def train_gen(hr, lr):\n","    with tf.GradientTape(persistent=True) as tape:\n","        fake= G(lr, training=True)\n","        fake_logits = D(fake, training=False)\n","        content_loss = mse_loss_fn(hr, fake)\n","        adv_loss = gen_loss(fake_logits)\n","        loss = content_loss + ad_loss_weight*adv_loss\n","    \n","    gen_grads= tape.gradient(loss, G.trainable_variables)\n","    gen_opt.apply_gradients(zip(gen_grads, G.trainable_variables))\n","    adv_grads = tape.gradient(adv_loss, G.trainable_variables)\n","\n","    return {\n","        'content_loss': tf.math.reduce_mean(content_loss), \n","        'adv_loss': tf.math.reduce_mean(adv_loss), \n","        'deep_grads': np.mean(gen_grads[-5]),\n","        'shallow_grads': np.mean(gen_grads[4]),\n","        'adv_deep_grads': np.mean(adv_grads[-3]),\n","        'adv_shallow_grads': np.mean(adv_grads[0])\n","    }\n","        \n","    \n","    \n","def train_disc(hr, lr):\n","    with tf.GradientTape() as tape:\n","        fake= G(lr, training=False)\n","        real_logits = D(hr, training=True)\n","        fake_logits = D(fake, training=True)\n","        loss = disc_loss(real_logits, fake_logits)\n","    \n","    disc_grads = tape.gradient(loss, D.trainable_variables)\n","    disc_opt.apply_gradients(zip(disc_grads, D.trainable_variables))\n","    \n","    return {\n","        'loss': tf.math.reduce_mean(loss),\n","        'deep_grads': np.mean(disc_grads[-1]), \n","        'shallow_grads': np.mean(disc_grads[0])\n","        \n","    }\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hXJJxkTds_LE","colab_type":"text"},"source":["# Evaluate Function"]},{"cell_type":"code","metadata":{"id":"bHSxwhYBs-hj","colab_type":"code","colab":{}},"source":["def evaluate():\n","    adv_loss= 0\n","    d_loss= 0\n","    content_loss = 0\n","    psnr = []\n","    ssim =[] \n","    step = 0\n","    for batch_hr, batch_lr in tqdm(val_ds.batch(batch_size), total= val_len//batch_size, unit=' batch',desc=f'Evaluating: '):\n","        fake= G(batch_lr, training=False)\n","        fake_logits = D(fake, training=False)\n","   \n","        c_loss = mse_loss_fn(batch_hr, fake)\n","        c_loss = tf.math.reduce_mean(c_loss)\n","        \n","        a_loss = gen_loss(fake_logits)\n","        t_loss = c_loss + ad_loss_weight*a_loss\n","\n","        real_logits = D(batch_hr, training=False)\n","        d_loss = disc_loss(real_logits, fake_logits)\n","        \n","        adv_loss += tf.math.reduce_mean(a_loss)\n","        d_loss += tf.math.reduce_mean(d_loss)\n","        content_loss += tf.math.reduce_mean(c_loss)\n","        \n","        step +=1\n","      \n","           \n","        fake= fake*0.5+0.5\n","        batch_hr= batch_hr*0.5+0.5\n","        p, s= compute_metrics(batch_hr, fake)\n","        p = tf.math.reduce_mean(p)\n","        s = tf.math.reduce_mean(s)\n","\n","        psnr.append(p)\n","        ssim.append(s)\n","          \n","        \n","    print(\"*-\"*25)\n","    mean_psnr = np.mean(psnr)\n","    mean_ssim = np.mean(ssim)\n","    adv_loss_mean = adv_loss/total\n","    disc_loss_mean = d_loss/total\n","    content_loss_mean = content_loss/total\n","    print(\"*-\"*25)\n","    print(\"disc loss = {:.3f} adv loss={:.3f}  content_loss = {:.3f}\\nOverall PSNR: {:.3f}, SSIM: {:.3f}\".format(disc_loss_mean,  adv_loss_mean,content_loss_mean, mean_psnr, mean_ssim))    \n","    print(\"*-\"*25)\n","\n","\n","    #############saving checkpoint#################\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HXhoAGnxFJjR"},"source":["# **Training Loops**"]},{"cell_type":"code","metadata":{"id":"oB1VmTOIBw_8","colab_type":"code","colab":{}},"source":["for epoch in range(last_epoch, iterations):\n","    adv_loss= 0\n","    d_loss= 0\n","    content_loss = 0\n","    psnr = []\n","    ssim =[] \n","    adv_sh_grads = 0 \n","    adv_deep_grads = 0 \n","    gen_sh_grads = 0 \n","    gen_deep_grads = 0 \n","    disc_sh_grads = 0 \n","    disc_deep_grads = 0 \n","    step = 0\n","    k = K\n","    checkout_interval = total//10\n","    for batch_hr, batch_lr in tqdm(train_ds.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE), total= total, unit=' batch',desc=f'Epoch {epoch}'):\n","        \n","        d = train_disc(batch_hr, batch_lr)\n","        if k == K:\n","          g = train_gen(batch_hr, batch_lr)\n","          k = 0\n","        adv_loss += g['adv_loss']\n","        d_loss += d['loss']\n","        content_loss += (g['content_loss'])\n","        adv_sh_grads += g['adv_shallow_grads']\n","        adv_deep_grads += g['adv_deep_grads']\n","        gen_sh_grads += g['shallow_grads']\n","        gen_deep_grads += g['deep_grads']\n"," \n","        disc_sh_grads += d['shallow_grads']\n","        disc_deep_grads += d['deep_grads']\n","        k+=1\n","        step +=1\n","        if step%(checkout_interval)== 0 : \n","            fake= G(batch_lr, training=False)\n","            fake= fake*0.5+0.5\n","            batch_hr= batch_hr*0.5+0.5\n","            p, s= compute_metrics(batch_hr, fake)\n","            psnr.append(p)\n","            ssim.append(s)\n","            print(f'\\n adv_loss: {adv_loss/step:.4f} content_loss: {content_loss/step:.4f}  disc_loss: {d_loss/step:.4f} \\n psnr: {np.mean(p):.4f}, ssim: {np.mean(s):.4f}')\n","            # sample_image(epoch, test_name, save_sample=False)\n","            # save_checkpoint(epoch, psnr_list, ssim_list, test_name)\n","        \n","    print(\"*-\"*25)\n","    print('GRADIENTS:')\n","    \n","    print('Gen gradients: shallow: {}   deep:{}'.format(gen_sh_grads/step, gen_sh_grads/step))\n","    print('disc gradients: shallow: {}   deep:{}'.format(disc_sh_grads/step, disc_sh_grads/step))\n","    print('adv gradients: shallow: {}   deep:{}'.format(adv_sh_grads/step, adv_sh_grads/step))\n","    mean_psnr = np.mean(psnr)\n","    mean_ssim = np.mean(ssim)\n","    adv_loss_mean = adv_loss/total\n","    disc_loss_mean = d_loss/total\n","    content_loss_mean = content_loss/total\n","    print(\"*-\"*25)\n","    print(\"disc loss = {:.3f} adv loss={:.3f}  content_loss = {:.3f}\\nOverall PSNR: {:.3f}, SSIM: {:.3f}\".format(disc_loss_mean,  adv_loss_mean,content_loss_mean, mean_psnr, mean_ssim))    \n","    print(\"*-\"*25)\n","\n","    #############saving checkpoint#################\n","    \n","    psnr_list.append(mean_psnr)\n","    ssim_list.append(mean_ssim) \n","    save_checkpoint(epoch+1, psnr_list, ssim_list, test_name)\n","    if epoch%interval == 0 :\n","        G.save('./models/{}/G_{}.h5'.format(test_name, epoch))\n","        D.save('./models/{}/D_{}.h5'.format(test_name, epoch))\n","\n","    if epoch%evaluation_interval==0:\n","      evaluate()\n","    sample_image(epoch, test_name, save_sample=True)"],"execution_count":null,"outputs":[]}]}